# 计数系统设计方案（位图 + 事件聚合 + SDS）

## 1. 目标与边界
- 统一支持内容实体的点赞、收藏等高并发计数；用户维度支持关注、粉丝、发文、获赞、获收藏。
- 写入需幂等、读低延迟、整体秒级最终一致；遇到异常可自动重建，保证数据纠偏能力。
- 成本与性能平衡：以内存占用低、CPU 分布均衡为目标，避免数据库热写与 Redis 哈希膨胀。

## 2. 架构总览
- 写路径（状态事实为位图，增量事件异步折叠到汇总）：
  - 位图切换（Lua 原子 `SETBIT`）→ 仅在状态变化时产出计数事件 → Kafka `counter-events` → 聚合桶（Redis Hash）持久化增量 → 定时刷写到 SDS 固定结构计数。
- 读路径（优先读汇总，异常时回溯事实）：
  - 常规读：直接读取 SDS 固定结构计数
  - 异常读：SDS 缺失或结构异常时，用位图分片 `BITCOUNT` 真实重建；分布式锁保护并清理对应聚合字段，避免重复加算。
- 用户维度计数通过关系 Outbox 事件处理器异步维护，并提供按需重建与抽样校验。

```
用户动作 → 位图切换(原子) → 计数事件 → Kafka
                                     ↓
                                聚合增量桶(Hash) ← 手动位点确认
                                     ↓ (fixedDelay=1s)
                             刷写到汇总(SDS) ← 原子 Lua 更新

读取计数：优先 SDS；异常：位图重建 + 分布式锁 + 清理聚合字段
```

核心代码参考：
- 位图切换与事件产出：`src/main/java/com/tongji/counter/service/impl/CounterServiceImpl.java:61-75`
- 聚合增量消费与刷写：
  - Kafka 增量入桶：`src/main/java/com/tongji/counter/event/CounterAggregationConsumer.java:33-46`
  - 定时刷写到 SDS：`src/main/java/com/tongji/counter/event/CounterAggregationConsumer.java:48-104`
- 读路径与异常重建：`src/main/java/com/tongji/counter/service/impl/CounterServiceImpl.java:77-125`
- 用户维度异步维护：`src/main/java/com/tongji/relation/processor/RelationEventProcessor.java:44-58`
- 灾备全量回放：`src/main/java/com/tongji/counter/event/CounterRebuildConsumer.java:35-53`

## 3. 数据模型与键设计
- 实体计数（内容维度）
  - 位图分片：`bm:{metric}:{etype}:{eid}:{chunk}`，分片位数 `32768`（4KB/分片）；`chunk=userId/32768`，`bit=userId%32768`（`BitmapShard.java:8-16`）。
  - 汇总计数（SDS 固定结构）：`cnt:{schema}:{etype}:{eid}`，`schema=v1`，段长 `FIELD_SIZE=4` 字节，段数 `SCHEMA_LEN=5`；大端 32 位整型编码（`CounterSchema.java:17-24`）。
  - 聚合增量桶：`agg:{schema}:{etype}:{eid}`（Hash），`field=idx`，`value=delta`（`CounterKeys.java:16-22`）。
  - 重建锁：`lock:sds-rebuild:{etype}:{eid}`，TTL 5s 防并发（`CounterServiceImpl.java:197-205`）。
- 用户计数（用户维度，SDS）
  - `ucnt:{userId}`，共 5 段×4 字节：
    1. 关注数 followings
    2. 粉丝数 followers
    3. 发文数 posts
    4. 获赞数 likesReceived
    5. 获收藏数 favsReceived
  - 增量原子折叠 Lua，1 基坐标（`UserCounterServiceImpl.java:31-59,97-122`）。
- Kafka 主题：`counter-events`（`CounterTopics.java:3-5`）。

## 4. 写路径详解
- 位图切换（幂等原子）：
  - Lua `TOGGLE_LUA`：当且仅当状态发生变化时置 1/清 0，返回 1 表示变更；否则返回 0（`CounterServiceImpl.java:249-265`）。
  - 成功变更后产出事件并发布到 Kafka，同步触发本地 Spring 事件供缓存失效等用途（`CounterServiceImpl.java:69-73`）。
- 聚合增量消费：
  - Kafka 消费计数事件，`HINCRBY agg:{schema}:{etype}:{eid} field=idx value=delta`；写桶成功后手动位点确认（`CounterAggregationConsumer.java:33-46`）。
- 定时刷写到 SDS：
  - 每 1 秒固定延迟扫描聚合桶键，原子执行 Lua `INCR_FIELD_LUA` 将增量折叠至对应 SDS 段并删除字段，避免重复加算（`CounterAggregationConsumer.java:48-104,106-136`）。
- 灾备全量回放（可选）：
  - `counter.rebuild.enabled=true` 时，使用 earliest 回放历史事件直接折叠到 SDS，确保在严重异常后可恢复（`CounterRebuildConsumer.java:15-21,35-53`）。

## 5. 读路径详解
- 常规读取（单 ID）：
  - `GET cnt:{schema}:{etype}:{eid}`，按照 Schema 偏移读取段值（大端 32 位），O(1)；若结构匹配则直接返回（`CounterServiceImpl.java:115-125,212-226`）。
- 异常重建：
  - SDS 缺失或长度异常时，先尝试获取分布式锁；对请求的指标逐一 `BITCOUNT bm:{metric}:{etype}:{eid}:*`（管道批量）求和，拼出新 SDS，回写并清理对应聚合字段（`CounterServiceImpl.java:87-110,228-247`）。
- 批量读取（Feed 场景）：
  - 管道批量 `GET` 多个 SDS 键，缺失时补 0，避免逐条 RTT（`CounterServiceImpl.java:127-163`）；
  - Feed 汇总与“是否点赞/收藏”判定集成：`src/main/java/com/tongji/knowpost/service/impl/KnowPostFeedServiceImpl.java:294-323`。
- 用户维度读取与校验：
  - `GET ucnt:{userId}`，异常时调用服务重建；抽样校验每 300s 对关注/粉丝做数据库对比，不一致则重建（`RelationController.java:105-178`）。

## 6. 一致性、幂等与容错
- 幂等来源：
  - 位图切换脚本仅在状态变化时返回成功，事件仅在变更时产出（`CounterServiceImpl.java:61-75,249-265`）。
  - 关系事件处理使用 Redis 去重键（TTL 10 分钟）保证幂等（`RelationEventProcessor.java:26-33`）。
- 最终一致：
  - 写入至聚合桶后至刷写的窗口为秒级（`fixedDelay=1000ms`），在窗口内读可能略滞后；异常情况下触发“事实重建”确保正确性。
- 原子折叠与上下限：
  - Lua 折叠时小于 0 归 0，避免负计数；当前采用 4 字节有符号上限，未来可平滑扩展段大小（`CounterAggregationConsumer.java:130-135` 与 `CounterSchema.java:17-24`）。
- 并发保护：
  - SDS 重建加分布式锁，避免并发回写；成功重建后清理对应聚合字段，杜绝重复计入（`CounterServiceImpl.java:87-110,197-205`）。
- 灾备：
  - 可切换重建消费者做全量事件回放；Kafka 生产端开启幂等与严格确认（参见 `application.yml` 的 producer 配置）。

## 7. 调度与配置
- 调度开关：`@EnableScheduling`（`src/main/java/com/tongji/counter/config/CounterConfig.java:14`）。
- 聚合刷写：`@Scheduled(fixedDelay = 1000L)`（`CounterAggregationConsumer.java:48-49`）。
- 热键窗口轮转（与 Feed 缓存相关）：`@Scheduled(fixedRateString = "${cache.hotkey.segment-seconds:10}000")`（`HotKeyDetector.java:68`）。

## 8. 方案对比
- 直接写 Redis Hash 计数（HINCRBY）
  - 优点：简单、读写一致；缺点：高写压、哈希膨胀、无“事实层”纠偏，幂等难以保证。
- 仅位图 + 读时 BITCOUNT
  - 优点：事实唯一可信；缺点：读时开销大（多分片 BITCOUNT），批量场景 RTT 与 CPU 高。
- 数据库计数列（UPDATE/乐观锁）
  - 优点：事务一致；缺点：数据库成为写热点，扩展性差，缓存回填复杂。
- 本方案（位图事实 + 事件聚合 + SDS 汇总）
  - 优点：写入幂等、读低延迟、具备事实回溯与自动纠偏、内存占用小、批量友好；
  - 代价：秒级最终一致、需要后台刷写与键空间管理。

## 9. 优势与取舍
- 高并发安全：Lua 原子切换 + 幂等事件，避免重复计入。
- 强纠偏能力：SDS 异常时可依据位图真实重建；灾备可事件回放。
- 低成本与高吞吐：SDS 固定结构（20 字节）提升读性能与局部性；聚合刷写降低写放大。
- 易扩展：`schema=v1` 可增加指标、调整段大小；键约定清晰易维护。
- 可观测与告警：刷写失败不提交位点；聚合桶残留即重试，便于埋点与监控。

## 10. 扩展性与演进路径
- 指标扩展：在 `CounterSchema.NAME_TO_IDX` 添加新映射并提高 `SCHEMA_LEN`，即可支持更多计数维度。
- 段大小演进：当 32 位可能溢出时，平滑切换至 5 字节或 8 字节实现（逐步迁移 schema）。
- 键空间索引：当前 `KEYS` 扫描（聚合桶与位图）在大键空间下存在风险，后续可引入索引集合登记活跃桶与分片，刷写与重建按集合遍历。
- 流式聚合：可升级为 Kafka Streams / Redis Streams 做准实时折叠，减少定时任务依赖。

## 11. 风险与边界
- 秒级一致性窗口：在 1s 刷写窗口内读到的 SDS 可能滞后；但位图事实不受影响，异常读可纠偏。
- KEY 扫描的可扩展性：生产环境需替换为索引集合或分区遍历，避免阻塞。
- 数据上限与异常：负数归零、溢出需要监控与迁移策略。
- 跨模块协同：关系事件去重窗口（10 分钟）需与业务幂等约定一致。

## 12. 接口与集成
- 行为接口：`/api/v1/action/like|unlike|fav|unfav`（`ActionController.java:26-68`）。
- 计数读取：`/api/v1/counter/{etype}/{eid}?metrics=`（`CounterController.java:21-38`）。
- 用户计数读取与抽样校验：`/api/v1/relation/counter`（`RelationController.java:105-178`）。
- Feed 集成：批量读取 SDS 并结合是否点赞/收藏（`KnowPostFeedServiceImpl.java:294-323`）。

## 13. 总结
- 以“位图为事实、SDS 为汇总、事件为桥梁”为核心思想，实现写幂等、读高效、异常可重建的计数系统。
- 方案在功能、性能与成本上取得平衡，具备演进空间；生产部署应完善键空间索引与监控体系，确保刷写与重建路径可观测、可告警、可回溯。